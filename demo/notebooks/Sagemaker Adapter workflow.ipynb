{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Hyper parameter tuning\n",
    "So in this part we will use Sagemaker's Hyper parameter optimization framework to tune our XGBoost Model. But, we will try to keep the workflow the same as previous example. To achieve this, lets create a simple workflow that adapts the inputs to match the format of Sagemaker and adapts the output of sagemaker to match the format of the example workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker integrations into Flyte - E.g. SagemakerXgBoostOptimizer\n",
    "Working with AWS we have developed a new plugin (more coming soon) to integrate various functionalities of Sagemaker directly into Flyte. Flyte makes it easy to orchestrate across different compute fabrics. One of them is Sagemaker.\n",
    "\n",
    "The example below shows a pre-created task that is available in flytekit, that allows using Sagemaker to tune XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Region': 'us-east-2', 'RoleArn': 'arn:aws:iam::236416911133:role/SageMaker-Executor', 'AlgorithmSpecification': {'TrainingImage': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:1', 'TrainingInputMode': 'File', 'AlgorithmName': 'xgboost'}, 'ResourceConfig': {'InstanceType': 'ml.m4.xlarge', 'InstanceCount': '1', 'VolumeSizeInGB': '25'}, 'StoppingCondition': {'MaxRuntimeInSeconds': '43200', 'MaxWaitTimeInSeconds': '43200'}}\n"
     ]
    }
   ],
   "source": [
    "from flytesagemakerplugin.sdk.tasks.plugin import SagemakerXgBoostOptimizer\n",
    "\n",
    "xgtrainer_task = SagemakerXgBoostOptimizer(\n",
    "    region=\"us-east-2\",\n",
    "    role_arn=\"arn:aws:iam::236416911133:role/SageMaker-Executor\",\n",
    "    resource_config={\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.xlarge\",\n",
    "        \"VolumeSizeInGB\": 25,\n",
    "    },\n",
    "    stopping_condition={\"MaxRuntimeInSeconds\": 43200, \"MaxWaitTimeInSeconds\": 43200},\n",
    "    algorithm_specification={\"TrainingInputMode\": \"File\", \"AlgorithmName\": \"xgboost\"},\n",
    "    retries=2,\n",
    "    cacheable=True,\n",
    "    cache_version=\"2.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker needs data in a specific format, so we can write a workflow to transform its inputs and outputs\n",
    "So let us create a pipeline that\n",
    "`Massages the input structured data frame into a format that Sagemaker likes`\n",
    " -> `Performs Model tuning with Sagemaker`\n",
    " -> `Untars model so that it matches our predict function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit.sdk.workflow import workflow_class, Output, Input\n",
    "from flytekit.sdk.types import Types\n",
    "from workflows.sagemaker_xgboost_hpo import example_hyperparams, xgtrainer_task, convert_to_sagemaker_csv, untar_xgboost\n",
    "\n",
    "@workflow_class\n",
    "class StructuredSagemakerXGBoostHPO(object):\n",
    "    # Input parameters\n",
    "    static_hyperparameters = Input(\n",
    "        Types.Generic,\n",
    "        help=\"A list of the static hyperparameters to pass to the training jobs.\",\n",
    "        default=example_hyperparams,\n",
    "    )\n",
    "    train_data = Input(\n",
    "        Types.Schema(), help=\"A Columnar schema that contains all the features used for training.\",\n",
    "    )\n",
    "    train_target = Input(\n",
    "        Types.Schema(), help=\"A Columnar schema that contains all the labeled results for train_data.\",\n",
    "    )\n",
    "\n",
    "    validation_data = Input(\n",
    "        Types.Schema(), help=\"A Columnar schema that contains all the features used for validation.\",\n",
    "    )\n",
    "    validation_target = Input(\n",
    "        Types.Schema(), help=\"A Columnar schema that contains all the labeled results for validation_data.\",\n",
    "    )\n",
    "\n",
    "    sagemaker_transform = convert_to_sagemaker_csv(x_train=train_data, y_train=train_target,\n",
    "                                                   x_test=validation_data, y_test=validation_target)\n",
    "\n",
    "    # Node definitions\n",
    "    train_node = xgtrainer_task(\n",
    "        static_hyperparameters=static_hyperparameters,\n",
    "        train=sagemaker_transform.outputs.train,\n",
    "        validation=sagemaker_transform.outputs.validation,\n",
    "    )\n",
    "\n",
    "    untar = untar_xgboost(\n",
    "        model_tar=train_node.outputs.model,\n",
    "    )\n",
    "\n",
    "    # Outputs\n",
    "    model = Output(untar.outputs.model, sdk_type=Types.Blob)\n",
    "\n",
    "# Create a launch plan that can be used in other workflows, with default inputs\n",
    "fit_lp = StructuredSagemakerXGBoostHPO.create_launch_plan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could run this Workflow on its own, but lets do something more interesting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awsdemo",
   "language": "python",
   "name": "awsdemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
